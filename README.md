# Neural Network from Scratch on the MNIST Dataset

- This project implements a simple 2-layer neural network from scratch using only NumPy.
- The neural network is trained and evaluated on the MNIST dataset, which consists of handwritten digits from 0 to 9.

Input Layer: 784 nodes (corresponding to the 28x28 pixel images)
Hidden Layer: 784 nodes with ReLU activation
Output Layer: 10 nodes with Softmax activation


## Activation Functions
- ReLU (Rectified Linear Unit): Used between the input layer and the hidden layer.
- Softmax: Used at the output layer to produce a probability distribution over the 10 classes.


Reference video: <a href = "https://youtu.be/w8yWXqWQYmU?si=fpy2NSayBfeK5Afm">Samson Zhang </a>

<a href = "https://www.kaggle.com/c/digit-recognizer/data?select=train.csv">Dataset</a>
